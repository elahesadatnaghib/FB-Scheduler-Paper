\documentclass[12pt]{aastex62}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
 
%\usepackage{multicol}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\makeatletter
\def\amsbb{\use@mathgroup \M@U \symAMSb}
\makeatother
\usepackage{bbold}
\usepackage{amsthm}
\usepackage{tabularx}
\usepackage[]{hyperref}
\usepackage{pbox}
\usepackage{dsfont}


%\renewcommand{\baselinestretch}{1.3}
\topmargin 0.0cm
\oddsidemargin 0.2cm
\textwidth 16cm 
\textheight 21cm
\footskip 1.0cm

\newtheorem{prop}{Proposition}
\theoremstyle{definition}
\newtheorem{defn}{Definition}

\DeclareMathAlphabet{\pazocal}{OMS}{zplm}{m}{n}
\DeclareMathOperator*{\argmin}{arg\,min}


\begin{document}
\title{A Framework for Telescope Schedulers: With Applications to the Large Synoptic Survey Telescope}

\author{Elahesadat Naghib}
\affil{Department of Operations Research and Financial Engineering, Princeton University, Princeton, NJ 08540, USA}
\email{enaghib@princeton.edu}

\author{Peter Yoachim} 
\affil{Department of Astronomy, University of Washington, Box 351580, U.W., Seattle, WA 98195, USA}
\email{yoachim@uw.edu}

\author{Robert J. Vanderbei} 
\affil{Department of Operations Research and Financial Engineering, Princeton University, Princeton, NJ 08540, USA}
\email{rvdb@princeton.edu}

\author{Andrew J. Connolly} 
\affil{Department of Astronomy, University of Washington, Box 351580, U.W., Seattle, WA 98195, USA}
\email{ajc26@uw.edu}

\author{R. Lynne Jones}
\affil{Department of Astronomy, University of Washington, Box 351580, U.W., Seattle, WA 98195, USA}
\email{ljones@astro.washington.edu}


%\begin{multicols}{2}

\begin{abstract}
{\bf How ground-based telescopes schedule their observations in response to different variables can significantly impact their efficiency. Namely, competing science priorities and constraints, variations in the weather, and the visibility of a particular part of the sky are some of the determining variables.\bf} In this paper we introduce the Feature-Based telescope scheduler which is an automated, proposal-free decision making algorithm. It offers \textit{controllability} of the behavior, \textit{adjustability} of the mission, and quick \textit{recoverability} from interruptions for large ground-based telescopes. By framing Feature-Based telescope scheduler in the context of a coherent mathematical model the functionality and performance of the algorithm is simple to interpret. Consequently adapting this scheduler for a broad range of ground-based instruments is straightforward. This paper presents a generic version of the Feature-Based scheduler, with minimal manual tailoring. We demonstrate its potential and flexibility to serve as a foundation for schedulers of the ground-based instruments. In addition, a modified version of the Feature-Based scheduler for the Large Synoptic Survey Telescope (LSST) is introduced and compared to the previous LSST schedulers.
\end{abstract}

\keywords{Artificial intelligence, autonomous telescope, LSST, reinforcement learning, scheduling, stochastic optimization}

\section{Introduction}
The Large Synoptic Survey Telescope (LSST), is a large,  ground-based optical survey that will image half of the sky every few nights from Cerro Pachon in Northern Chile. The LSST comprises an 8.4 meter  primary mirror and a 3.2 Gigapixel camera.  With a 9.6 $\text{degree}^2$ field-of-view. It will visit each part of its 18000 $\text{degree}^2$ primary survey area about 1000 times over the course of 10 years. Each visit will likely comprise a 15 second pair of exposures with a single visit depth of about 24.5 magnitudes (AB) (in the six bands u, g, r, i, z, and y). The revolutionary role of this telescope calls for no less than optimal operation. 

The algorithm that makes the sequential decisions of which filter to use, and which direction to point the telescope to, is called \textit{scheduler}. A scheduler has to maximize the scientific outcome of the telescope during its limited period of operation. 

There are four primary science drivers for the LSST project: the characterization of dark energy through the multiple cosmological probes (e.g. gravitational weak lensing, luminosity distances from Type Ia supernovae, and Baryon Acoustic Oscillations); mapping the 3D distribution of stars within our Galaxy; a census of solar system objects within the Solar System; and a detailed study of the transient and variable universe. Each of these objectives has a different set of constraints and requirements on how the observations are made (e.g. the cadence of the observations, the number of filters as a function of time, the acceptable airmass range for an observation). Generally, the mission of modern, large, ground-based telescopes, such as LSST, is constrained with various stochastic factors, and contains competing objectives which are vastly different in nature, due to the different nature of the scientific expectations. In this paper, we propose a framework to formulate the problem of scheduling for the new generation of ground-based telescopes then introduce a scheduler based on the proposed model.

The first generation of schedulers for astronomical instruments were developed for space missions mainly to automate their operation. ROSAT mission's scheduler in 1990 \citep{nowakovski1999using}, Spike \citep{johnston1994spike}, Hubble Telescope's scheduler in 1994, and HSTS \citep{muscettola1995automating} in 1995 pioneered many of the developments in algorithmic scheduling of observations for the space missions.

Despite the similarity of the science objectives for space and ground-based telescopes, the determining factors for the purpose of scheduling are fundamentally different. While space telescopes are required to respect kinematical and dynamical constraints, weather is the main challenge in the scheduling of ground-based telescopes. The former is predictable and efficiently computable, while the later involves both inherent uncertainties, and uncertainties due to computational limitations.

Earlier algorithmic approaches to the scheduling of ground-based telescopes are heavily based on observation \textit{proposals}. Proposals are hand-crafted sequences of scripted astronomical observations. They are generally tested only for feasibility (e.g. that a set of fields were visible, or lie within a specified airmass range, or within a window in time), but not necessarily for optimality. For instance, the operation of Keck Telescope, 1993 \citep{nelson1985design}, is 100\% based on proposals, and the Hobby-Eberly Telescope, 1997 \citep{shetrone2007ten}, has a semi-manual scheduling scheme.

More recently, the development of more expensive ground-based instruments with complex missions made it impossible to rely solely on hand-crafted proposals. The need for more efficient use of the instrument's time led to the development of decision making algorithms to optimize their science output. {\bf For instance, the scheduler of the Liverpool robotic telescope was designed in 1997 to automatically allocate time slots to chunks of scripted observations. The time allocation strategy was preferred to the scheduling at the single visit level. The scheduling at the single visit level is referred to as optimal scheduling by the authors \citep{steele1997control} where it is stated that the optimal scheduling requires to reevaluate the future sequence of observations once it is interrupted, but the necessary extra computation is neither affordable nor fast enough.\bf} However, in this paper, we show that the scheduling in the single visit level, optimal scheduling, can be quickly recovered after an interruption, if a memoryless framework is used. Thus the optimality is not necessarily needed to be sacrificed because of the limited computational resources. Another example is the Las Cumbres Observatory, Global Telescope Network (LCOGT) with one of the most advanced telescope scheduling algorithms \citep{Boroson14, Saunders14}. LCOGT uses an integer linear programming (ILP) model, solved with the Gurobi algorithm \citep{gurobi}, to optimize the scheduling of observations over a global network of telescopes \citep{Lampoudi15}. Due to the success of this approach the Zwicky Transient Facility at Palomar Observatory \citep{Bellm14} has also adopted an ILP scheduler\footnote{GitHub repository: \url{https://github.com/ZwickyTransientFacility/ztf_sim}}. The ILP scheduling model performs well for observatories where slew overheads are small compared to exposure execution time. LCOGT is able to schedule observations in long contiguous blocks. In contrast, the LSST plans to have observations of about 30 seconds with slew times up to 2 minutes (when there's a filter change), hence it requires a scheduling algorithm that can explicitly minimize the slew times between successive observations. One could use the ILP scheduling approach to schedule large scripted blocks of observations for LSST. The blocks could be set to follow a path that only includes short slews (this is similar to the strategy taken in the scheduler developed by D. Rothchild et al.). The disadvantage of this approach and any other scripted schedule is principally the lack of recoverability from unpredictable intruptions, such as inability to dodge the clouds.

Even given the reliability of fully automatic scheduling technologies, remain a number of modern telescopes such as SALT, 2005 \citep{brink2008salt}, and ALMA, 2013 \citep{wootten2003atacama}, which are being operated based only on traditional hand-crafted proposals. ALMA in particular, requires a highly regulated structure for proposals that potentially leads to suboptimality as discussed in \citep{alexander2017enabling}, where the authors suggest a number of corrections for the scheduling regulations to provide adaptivity to time-sensitive observations.

The LSST community, however, in addition to a proposal-based scheduler, introduced in \citep{delgado2016lsst}, supported the design and implementation of proposal-free decision algorithms, such as the Feature-Based scheduler, first introduced in \citep{naghib2016feature}, as well as a semi-scripted cadence by D. Rothchild et al. (2018, in preparation).

In Section~\ref{sec_SM}, first we explain the choice of Markovian framework for the Feature-Based scheduler, and in Sections~\ref{sec_Markov} and~\ref{sec_Markov_approx} we provide the mathematical details of the scheduler model in that framework. Section~\ref{sec_opt} presents two approaches for the optimization of the model's parameters. Sections~\ref{sec_lsst_problem} demonstrates the application of the Feature-Based scheduler on the LSST which is then followed by a comparison between a modified version of the Feature-Based scheduler and LSST's current official scheduler in Section~\ref{sec_comp}. Finally, Section~\ref{sec_conclusion} presents our concluding remarks.


\section{Scheduling framework}\label{sec_SM}
To run a ground-based telescope with multiple science objectives, such as LSST, the scheduler has to offer \textit{controllability}, \textit{adjustability}, and \textit{recoverability}.
\begin{itemize}
\item \textbf{Controllability}: {\bf A telescope is controllable by a given scheduler if change of the scheduler's design parameters are visibly manifested in the behavior and performance of the telescope. Controllability is determined by two factors, the information that is fed to the scheduler and the structure of the scheduler. Feeding the scheduler with information that is not sufficiently relevant to the high-level mission objectives leads to an output schedule that is irrelevant to the performance of the telescope. In addition to provide relevant information, a scheduler with a flexible structure is needed to cover a large class of decision strategies, each determined by a set of its design parameters. Controllability is necessary for a scheduler to be optimizable by searching within the space of its design parameters.\bf}

\item \textbf{Adjustability}: A scheduler must be adjustable according to new conditions, environment and scientific desirables. For a complex and multi-objective mission it is common that the scientific goals are required to be modified in the middle of the operational period. Another example is aging that causes changes in the mechanical characteristics of the telescope. Regardless of the reason, {\bf A scheduler is adjustable if the changes can be accomodated with reasonable computational cost, and preferably no or minimal human-expert intervention. Hand-tuned scheduling strategies and policies that are written in forms of instructions for human observation proposal writers, are not fully adjustable.\bf}

\item \textbf{Recoverability}: The presence of unpredictable factors in the operation of ground-based telescopes are due to the natural stochastic processes (such as the weather), and complexity of the mechanical facility. Unscheduled downtime and instrument failures are examples of the many unpredictable survey interruptions. In addition, there are inherently predictable interruptions, such as maintenance downtimes, and cable winding that due to the complexity of the mechanical system are not computationally affordable and/or valuable to keep track of. Therefore, they are considered to be stochastic variables as well. A scheduler is required to be able to make alternative decisions once a previously unpredictable event occurred in order to maintain the optimality of the operation. Moreover, it has to return to its normal behavior shortly after the interrupting event is over. {\bf Such a scheduler is called recoverable and its response to the interruptions is ideally as quick as the length of a single observation. The response time is usually limited by computational complexity of the scheduling algorithm. For instance, strategies that need to look back at history of observations or look forward through possible sequences of observations are not fast recoverables due to their computational complexity. Also, scripted sequences of observations, such as a proposals, completely lack the attribute of recoverability.\bf}
\end{itemize}

{\bf The Feature-Based scheduler is verifiably controllable. The Markovian framework provides a well-defined and tractable set of design parameters, and a well-defined measure of the performance for any given choice of the design parameters, therefore controllability is empirically verifiable. It is adjustable because the derivation of the design parameters are automatic and computationally tractable, once a new high-level mission objective is given, or the telescope-environment system changes. Finally it is recoverable due to the inherent memorylessness of the Markovian Decision Process which for a decision at any time, only requires the current state of the system.\bf}

\subsection{Markovian representation}\label{sec_Markov}
\begin{defn}
Let $X_{(.)}$ be a stochastic process for which $X_i$ represents the state of the system at $t_i$, and $\pazocal{S}$ be the set of all possible states that the system can take. Let $\mathbb{P}(X_i)$, be the probability distribution of $X_i$ on $\pazocal{S}$. Then $X(.)$ is a Markovian process, if and only if, it satisfies the following \textit{memorylessness} property,
\begin{equation*}
\forall i ~~~~~~~\mathbb{P}(X_{{i+1}} | X_{i}) = \mathbb{P}(X_{{i+1}} | X_{i}, X_{{i-1}},\dots, X_{0}),
\end{equation*}
where $\mathbb{P}(X_{{i+1}} | X_{i})$ is the conditional probability distribution of the system's state at $t_{i+1}$ given it's state at $t_i$, and $\mathbb{P}(X_{{i+1}} | X_{i}, X_{{i-1}},\dots, X_{0})$ is the conditional probability distribution of the system's state at $t_{i+1}$, given all of the states that the system has been in until $t_i$.
\end{defn}
Memorylessness property asserts that the system's next state depends only on its current state and is independent of its earlier history. This property is the main reason for choosing a Markovian framework for the scheduler. 
\begin{defn}
Let $<\pazocal{S},\pazocal{A}, P_a(.,.), R_a(.,.), \gamma>$, be a \textit{Markovian Decision Process (MDP)}, where $\pazocal{A}$ is the set of actions, and $P_a(x, y)$ transition probability from state $x$ to $y$ which is equal to $\mathbb{P}(X_{i+1}=y | X_i=x, a)$, the conditional probability of transition from state $x$ to state $y$ given action $a \in  \pazocal{A}$. Finally the transition reward is denoted by $R_a(x,y)$, and $\gamma \in (0,1]$ is the discount factor.
\end{defn}
\begin{defn}
Action $a_{i} \in \pazocal{A}$ is \textit{admissible} for $<\pazocal{S},\pazocal{A}, P_a(.,.), R_a(.,.), \gamma>$, {\bf if it is \textit{feasible}, (i.e. it is possible to be taken at $t_i$), and \textit{progressively measurable}, (i.e. depends only on the current state of the system, $X_i$, neither on the future states, nor on any information about the system's history)\bf}.
\end{defn}
To schedule a telescope is to take an admissible action (e.g. to determine the next observation) at all decision steps $t_i$. Notice that the decision steps $t_i$, are not uniformly spaced, and are determined by the time that each observation takes. {\bf Also note that the scheduling is a finite time procedure from $t_0$, until the operation of the telescope is over, $T$. Our framework is general and covers all possible discretizations of the time in both finite and infinite horizon scheduling tasks.\bf}

\begin{defn}
A \textit{deterministic policy} $\pi : \pazocal{S} \rightarrow \pazocal{A}_i$, is a mapping from $\pazocal{S}$ to the set of all admissible actions at $t_i$, denoted by $\pazocal{A}_i$. 
\end{defn}
%
A policy provides a time-invariant law that for all possible $x_i \in \pazocal{S}$ suggests an admissible action. The policy is the heart of the scheduler which takes the necessary information encoded in the current state and makes a decision for the next observation. Design of an optimal scheduler is mainly to find an optimal policy.

\begin{defn}
\textit{A deterministic optimal policy} $\pi$ is a solution to the following optimization problem,
\begin{equation}\label{equ_opt1}
\begin{aligned}
& \underset{\pi}{\text{maximize}}
& & E_{\pi}[\sum_{i = 0}^N \gamma^i R_{\pi(X_{i-1})}(X_{i-1}, X_{i}) | x_0],
\end{aligned}
\end{equation}
where $x_0$ is a given initial state.
\end{defn}
%
With this definition, we take the optimal policy to be a policy that maximizes the expected discounted sum of the rewards. The discount factor, $0 < \gamma \leq 1$, determines the priority of the overall gain (after an episode of observation) versus instant gains (after a single observation). Larger discount factors prioritize overall gains over instant gains. The choice of $\gamma$ depends on the application and is empirically tuned. 
%
\begin{prop} \label{prop_main}
For the Markov decision process of $<\pazocal{S},\pazocal{A}, P_a(.,.), R_a(.,.), \gamma>$, there exists a deterministic optimal policy, and it can be written as follows, 
\begin{equation}\label{equ_opt_pol}
\pi^*= \argmin_{a_{i} \in \pazocal{A}_i} E[\Phi(X_{{i+1}}) | a_{i}],
\end{equation}
where $\Phi: \pazocal{S} \rightarrow \rm I\!R$ is a function of the following form,

\begin{equation}\label{equ_phi_compatible}
\Phi(x_{i}) =  - R_{\pi^*(x_{i-1})}(x_{i-1},x_i)  + \gamma E_{\pi^*}[\Phi(X_{{i+1}})|x_i].
\end{equation}
\end{prop}

\textit{Proof.} See Appendix.

For the telescope scheduler we require the policy to be deterministic, because the simulations have to be repeatable for comparison and evaluation purposes. However it can be shown that the deterministic optimal policy is not only optimal amongst deterministic policies, but also is optimal amongst stochastic policies. Therefore the choice of deterministic policy does not harm the optimality of the scheduler. 

As a result of Proposition~\ref{prop_main}, search for the optimal policy of Problem (\ref{equ_opt1}), can be reduced from search over the set of policies (all possible mappings from the state-space to the action-space) to search over $\Phi$ functions, without loss of generality. This is a significant reduction made possible by the choice of Markovian Decision framework.

\subsection{Markovian approximation}\label{sec_Markov_approx}
For a decision that is inherently time dependent, such as scheduling an observation, only a maximal definition of the system's state yields a perfect Markovian system. The maximal definition of state-space includes all of the possible decision sequences. In particular, LSST requires a sequence of about 1000 decisions at each night. Therefore, storing all possible scenarios requires a state space of size $N_{f}^{1000}$, where $N_f$ is the number of tessellation centers on the visible sky. For any $N_f$, $N_{f}^{1000}$ number of scenarios is neither tractable nor storable in a realistic memory. In order to overcome the curse of dimensionality, we have designed a set of \textit{features} to summarize the state of the system with only the most determining information. Thus, the system of telescope-environment is only an approximated Markovian system once its state-space is reduced to a \textit{feature-space}. 

On the other hand, Proposition (\ref{equ_opt1}) shows that the search for an optimal scheduler lies within the set of functions instead of a much larger set of mappings from the state-space to the action-space. Despite this reduction, Problem (\ref{equ_opt1}) is still an infinite dimensional optimization problem, because its variable is a function. To be able to numerically compute the $\Phi$ function, we assume that it can be expressed as a linear weighted summation of some basis functions. With this structure of $\Phi$, our optimization would be further reduced to find the optimal values of the weights.
%
\begin{equation*}
 \tilde{\Phi}_{\theta}({x}_{i}) := \sum_{j=1}^m \theta_j \Phi_j(x_{i}),
\end{equation*}
%
where $\theta$ is the vector of variables which fully characterizes $\tilde{\Phi}(.)$, and $\Phi_i(x_i)$'s are the building blocks of $\tilde(\Phi)$. We refer to  $\Phi_i(x_i)$'s as \textit{basis functions} which are hand-crafted functions of the features, and are designed in such a way that the domain-knowledge of astronomical observation is incorporated into the decision making strategy.

With this approximation, search-space is reduced from the space of functions to a finite dimensional vector space. This approximation substitutes the original optimal policy (\ref{equ_opt_pol}) with the following approximate policy,
%
\begin{equation}\label{equ_approx_pol}
\begin{aligned}
\tilde{\pi}_{\theta}^*(x_{i})& = \argmin_{a_{i}} E[ \tilde{\Phi}_{\theta^*}(X_{i+1}) | a_{i}] =  \argmin_{a_{i}} \sum_{j=1}^k \theta^*_j E[\Phi_j(X_{i+1}) | a_{i}],\\
\end{aligned}
\end{equation}
%
where $\theta^*$ is a solution to the following optimization problem, in which policy $\pi$ is fully determined by $\theta$.
%
\begin{equation}\label{equ_opt3}
\begin{aligned}
& \underset{\theta}{\text{maximize}}
& & E_{\pi_{\theta}}[\sum_{i=0}^N \gamma^i R_{\pi_{\theta} (X_{i-1})}(X_{{i-1}}, X_{i}) | x_0].
\end{aligned}
\end{equation}

{\bf Note that the computational time to find the optimal policy could be relatively long, but it is an offline task and can be done before the telescope starts to operate. However, evaluating the action, given a policy at all $t_i$ must be at least as fast as the length of the shortest observation. A linear policy not only demands relatively small computational resources to be optimized, but also is very quick to be evaluated in real time.\bf}

\section{Scheduler optimization}\label{sec_opt}
In this section we introduce two different approaches to solve Problem (\ref{equ_opt3}). The solution of this problem is an optimal set of weights, $\theta^*$, for a given set of basis functions. The first optimization approach is faster, but requires the high-level mission objectives to belong to a certain class of functions. The second optimizing approach is applicable to all types of the high-level mission objectives, however requires more computational resources. 

\subsection{Reinforcement Learning}
Assume that there exists a well-defined notion of an instant reward for each state transition, then $\Phi(x_{{i}})$ by definition given in Equation (\ref{equ_phi_compatible}) is,

\begin{equation} \label{equ_learning1}
\Phi(x_{i}) = - R_{\pi^*(x_{i-1})}(x_{i-1},x_i)  + \gamma E_{\pi^*}[\Phi(X_{{i+1}})|x_i].
\end{equation}

Accordingly, for the parameterized $\Phi$ function, we require the following,

\begin{equation} \label{equ_learning2}
\begin{aligned}
\tilde \Phi_{\theta^*}(x_{i}) &=  - R_{\pi_{\theta^*}(x_{i-1})}(x_{i-1},x_i)  + \gamma E_{\pi_{\theta^*}}[\tilde \Phi_{\theta^*}(X_{{i+1}})|x_i].\\
\end{aligned}
\end{equation}

The main idea behind reinforcement learning is to optimize $\tilde {\Phi}_{\theta^*}(.)$ while the decisions are being made in a simulated environment. First, the policy is initialized by an arbitrary set of variables, $\theta^0$. Then the first decision is made by the policy associated with $\theta^0$. Then based on the outcome reward, we update the initial set of variables to get $\theta^1$. Next, decision is made with a policy associated with the new set of variables $\theta^1$ which yields a reward. By repeating this process, we gradually update the variables in each decision step until they converge to an optimal value. The optimal $\tilde {\Phi}$ respects Equation (\ref{equ_learning2}) for all $i \in \{j: t_0 \leq t^j \leq T\}$.

Note that at $t_i$, after the transition from $x_{{i-1}}$ to $x_{{i}}$ we have the value of $R_{\pi_{\theta^i}}(x_{i-1},x_i) $ already evaluated for making the decision. On the other hand, $\Phi_{\theta^i}(x_{i})$ can be approximated by $E_{\pi_{\theta^i}}[ \Phi_{\theta^i}(x_{i})]$ which is also evaluated in the decision making process, where $\theta^i$ is the last version of the optimization variables at $t_i$. Using the target value given in Equation (\ref{equ_learning2}), the update rule is as follows,

\begin{equation}\label{equ_updatePHI}
\begin{aligned}
\tilde \Phi_{\theta^{i+1}}(x_{{i}}) = (1-\alpha) \tilde \Phi_{\theta^{i}}(x_{{i}})+ \alpha  ( \gamma  E_{\pi_{\theta^i}}[\tilde \Phi_{\theta^{i}}(x_{{i+1}})|\pi_{\theta^{i}}(x_{{i}})] -R_{\pi_{\theta^i}}(x_{i-1},x_i) ),
\end{aligned}
\end{equation}
in which, $0<\alpha<1$ is the learning rate. The first term in the right hand side of the equation is the most recent approximated value of $\tilde \Phi$ associated with $\theta^i$. This term has $(1-\alpha)$ amount of contribution to $\tilde \Phi$ of the next iteration. The second term is the value of the target $\tilde \Phi$ (according to Equation (\ref{equ_learning2}) which has $\alpha$ amount of contribution to $\tilde \Phi$ of the next iteration. Clearly with smaller $\alpha$'s this update imposes smaller adjustments from one decision to the next decision. {\bf The choice of $\alpha$ depends on the application. Higher $\alpha$'s are computationally preferred because they speed up the optimization, however depending on the natural dynamic of the system and how much it changes from one decision to another, higher $\alpha$'s could make the process of optimization unstable, and diverging. \bf} Accordingly, updates of the variables $\theta_j,~~j=1,\dots, k$ can be expressed as follows, 

\begin{equation} \label{equ_TD_update}
\begin{aligned}
\theta_j^{i+1} &= \theta_j^{i} + \Big( \tilde \Phi_{\theta^{i+1}}(x_{{i}})  - \tilde \Phi_{\theta^{i}}(x_{{i}}) \Big)\Phi_j(x_{i})\\
& = \theta_j^{i} + \alpha \Big(-\Phi_{\theta^{i}}(x_{{i}}) + \gamma  E_{\pi_{\theta^i}}[\tilde \Phi_{\theta^{i}}(X_{{i+1}})|x_{{i}}] -R_{\pi}(x_{i-1},x_i)  \Big)\Phi_j(x_{i}).
\end{aligned}
\end{equation}

This variant of reinforcement learning is called \textit{Temporal-Difference} (TD) learning with function approximation \citep{tsitsiklis1997analysis}. Variants of this approach have been successfully applied to real-life problems such as training of a backgammon player \citep{tesauro1995temporal}.

Recall that in order to be able to use the TD reinforcement learning method, it is necessary to have a well-behaved notion of a reward that reflects the instant gain of all possible decisions at all of the decision steps. Moreover, the discounted sum of the instant rewards has to reflect the objective of the mission according to Equation (\ref{equ_opt1}). For instance, in the LSST scheduling problem, after each visit, the negative of the slew time, is a well-defined instant reward that reflects how time-efficiently the telescope is being operated. This however does not reflect all aspects of the high-level mission's objective such as the need to re-observe a field within a valid time window (explained in Section~\ref{sec_lsst_problem}), for which there is no equivalent instant reward. For this reason, we also introduced a black-box function optimizer in the following section. We use the black-box approach when there is no well-defined notion of an instant reward with which we can build the high-level mission's objective function.

\subsection{Global Optimization}\label{sec_gopt}
In the absence of a well-defined instant reward, instead of solving problem~(\ref{equ_opt3}), The following problem can be solved,
%
\begin{equation}%\label{equ_opt4}
\begin{aligned}
& \underset{\theta}{\text{maximize}}
& & U_{\pi_{\theta}}(x_i,x_{i+1}, \dots, x_{j}),
\end{aligned}
\end{equation}
%
where $U_{\pi_{\theta}}(x_i,x_{i+1}, \dots, x_{j})$ is a utility function that measures the performance of the scheduler on a simulated episode of the operation from $t_i$ to $t_j$. Unlike the previous method, the policy, $\pi_{\theta}$, is fixed during an episode of simulation in this approach. The simulation episode is usually a small fraction of the total time of the telescope's operation, $T - t_0$. The idea is to test the performance of a set of candidate policies in a parallel manner, within a short episode. Then infer a better set of candidates and repeat until the performance cannot be improved. In general, $U(.)$, can not be explicitly expressed as a function of $\theta$, therefore a global optimizer that can maximize a black-box function is required. Evolutionary optimizers have successfully been applied to numerous real-life problems involving black-box function optimization, and specifically astronomical mission planning such as the scheduling of Exoplanet Characterisation Observatory \citep{garcia2015artificial}. We used the $e$DE evolutionary optimizer \citep{naghib2016entropic} which is an adaptive version of the Differential Evolution (DE) algorithm (\citep{storn1997differential}). DE is generally one of the most efficient evolutionary algorithms and the $e$DE variant uses a notion of entropy to automatically preserve the diversity of the candidate solutions. As a result, in contrast with DE, it does not require the user to choose any tuning parameters for the algorithm, which is the most time-consuming task in using an evolutionary optimizer. In addition, $e$DE, similar to any other evolutionary algorithm is highly parallelizable, and the computational time can be almost linearly decreased with respect to the number of computational cores.

\section{Problem of Scheduling for the LSST}\label{sec_lsst_problem}
%
\begin{table}[h]
\caption{Key terms and notations used in the definition of the features and the basis functions} \label{tab_notatopn}
\begin{tabular}{l  l }
\hline
fields & fixed point configuration on the sky such that the visible sky could be completely covered by pointing\\
& the telescope toward all of those directions, \\
$N_{f}$ & total number of the fields,\\
$t$& Coordinated Universal Time (UTC),\\ 
$\tau_s(t)$ ($\tau_e(t)$)& beginning (end) of the night that $t$ lies within,\\
$\tau_{rise\text{(}set\text{)}}(i,f,t)$ & rising (setting) time of field-filter $(i,f)$ above (below) the acceptable airmass horizon at current night,\\
$\tau_n(i,f,t)$& time of the last visit of field-filter $(i,f)$ before $\tau_s(t)$,\\
$id(t)$ & ID number of the field that is visited at $t$,\\
$ft(t)$& camera's filter at $t$,\\
$n(i,f,t)$ & total number of the visits of field-filter $(i,f)$ before $t$,\\
$slew(i,j)$& slew time from field $i$ to field $j$ in seconds,\\
$settling(i,j)$& mechanical settling time after slewing from field $i$ to field $j$,\\
$\Delta t_{f}$& time needed to change filter, a constant value about 2 minutes,\\
$t_{dome}(i)$& time needed to move the dome to make field $i$ visible to the telescope,\\
$ha(i,t)$ & hour angle of the center of field $i$ at $t$ in hours, $-12 \leq ha(i,t) \leq 12$,\\
$am(i,t)$ & airmass of the center of field $i$ at $t$,\\
$br(i,t)$ & brightness of the sky at the center of field $i$ at $t$,\\
$\sigma(i,t)$ & seeing of the sky at the center of field $i$ at $t$,\\
$K(i,f, t)$ & atmospheric extinction coefficient\\
$W_1, W_2$ & given constant time window within which a revisit is valid, \\
%$I_{survey}$ & set of all fields in $survey \in \{WFD,DD,NES,GP,SCP\}$,\\ 
\hline
\end{tabular}
\end{table}

The LSST's mission is to uniformly scan the visible sky within 5 different regions shown in Figure~\ref{fig_proposals}. Each region, also referred to as \textit{survey}, has certain science-driven goals and constraints, defined and precisely described in \citep{ivezic2008large}.  
%
\begin{figure}[h!]
\begin{center}
%\includegraphics[width=.6\linewidth]{minion_1016_proposal_footprint.pdf}
\epsscale{0.5}
\plotone{minion_1016_proposal_footprint.pdf}
\caption{Regions of the sky with different requirements and constraints for scheduling: (1) Galactic Plane Region (GP), (2) Universal or Wide Fast Deep (WFD), (3) South Celestial Pole (SCP), (4) North Ecliptic Spur (NES), and (5) Deep Drilling Fields (DD).}
\end{center}
\end{figure}\label{fig_proposals}
%
The notion of the features, enables the scheduler to systematically fetch all of the various information and turn them into comparable quantities for the purpose of the decision making. The proposed feature-space of the LSST contains seven features, each can be evaluated given a field $i$, a filter $f$, and a time $t$. The fields discretize the visible sky through a fixed sky tessellation. There are 6 choices of filter, $[u,g,r,i,z,y]$, for each visit. And finally the time is discretized by the natural timing of the process. In other words, the time interval between two decision steps is the time that it takes to execute the observation in between the two decisions. Given that a consecutive visit of the same field-filter is not allowed in the main survey, there is a slew time between any two decisions, therefore $t_{j} - t_{j-1} > 0$. On the other hand, the operation is over a limited time horizon, $T$, thus the number of the decision time-steps is finite. In conclusion, a finitely discretized sky, a finite number of the filters and a finite number of the time-steps pose a finite feature-space, denoted by $\{(f_1(i,f,t_j)\dots f_7(i,f,t_j)): i = 1\dots n_f, f\in \{u,g,r,i,z,y\}, j = 0,\dots N\}$. With this feature-space the implication of the policy, stated in Equation (\ref{equ_approx_pol}), is as follows,
%
\begin{equation}\label{equ_approx_pol_imp}
\tilde{\pi}^*(x_j)= \argmin_{(i,f)\in \pazocal{A}_{j}} \sum_{k=1}^5 \theta^*_k E_{\pi}[\Phi_k(X_{j+1}) | x_j],\\
\end{equation}
%
where $x_j = [f_{1, \dots,7}(id(t_j),ft(t_j),t_j)]$ is a 7-dimensional state at $t_j$, and $(i,f)$ is an admissible pair of field-filter. Section~\ref{sec_cstr} introduces the constraints under which a field-filter pair is admissible at $t_{j}$. Accordingly, $\pazocal{A}_j$ is the set of all field-filter pairs that are admissible at $t_j$.

In the implementation of the scheduling software we took a modular approach. The expected values of the basis functions, $E_{\pi}[\Phi_k(x_{j+1})|x_j]$ for $k=1,\dots, 5$, are evaluated in separate software routines. Then they are delivered to the scheduler at the stage of the decision. The basis functions that address the environmental parameters are developed by the LSST community. For example, see \citep{2014SPIE.9145E..1AG} for the parameters that capture the status of the LSST site, \citep{sebag2008lsst} and \citep{sebag2007lsst} for cloud cover measurements that were used to develop a predictive cloud model, and see \citep{yoachim2016optical} for the sky brightness model. 

Generally speaking, making a decision for a visit at $t$ for the LSST scheduling problem is mainly determined by the following factors,
\begin{enumerate}
\item The amount of the time it takes to redirect the telescope and the dome to move from one target to the next target.
\item The short-term science-driven requirements, such as the same-night revisit of a field.
\item The long-term mission-driven requirements, such as maintaining a uniform coverage of all field-filter pairs within each region.
\item {\bf The observational quality of a field-filter at the time of decision, such as the expected depth of the resulting image. \bf}
\item The general preference for observing the fields around the meridian.
\end{enumerate}

Accordingly, the basis functions of the LSST scheduler, are designed to formalize the above factors. For the full definition of the basis functions of the Feature-Based scheduler for LSST refer to Section~\ref{sec_lsst_bfs}.

In what follows, first we precisely define the features and basis functions of the LSST scheduler, then we show how the two optimization procedures, described in Section~\ref{sec_opt}, are applied. The optimization results are associated with two sample objective functions. However, the LSST community, and principally any individual can design their own mission objective function, and repeat the optimization procedure to obtain a scheduler that is optimizes the probabilistic expectation of their objective function.

\subsection{Features of the telescope scheduler}\label{sec_lsst_features}
For designing the features, it is important to avoid redundancy in the information which features contain. It is also critical to hold a modular approach in the delivery of the information to the decision stage. For instance, consider the amount of time, $\Delta t$, it takes for a telescope to move from a visit to another. In the LSST problem, $\Delta t$ mainly depends on the slew time, mechanical settling time, the dome placement time, and the time it takes to change the filter. All of these timings are available through a precise simulation of the LSST model \citep{delgado2014lsst}. A modular design would be to bring the summation of the operational timings to the stage of the decision instead of bringing them separately as different features. This approach makes the implementation significantly simpler, and more readable. Conceptually, modular approach makes it possible to track the effect of the operational costs of a similar nature in the overall performance of the scheduling. Particularly, the operational cost is independent of the amount to which each cause contributes to the overall $\Delta t$. Hence, bringing the timing of each procedure separately in the decision making level adds unnecessary complications to the design.

This section proposes seven features for the description of the LSST-environment state in Table \ref{tab_features}, with the key terms and notations defined in Table \ref{tab_notatopn}. Features are designed to efficiently carry the determining information with a modular approach. Each feature is denoted by $f_k(i,f,t)$ for $k= 1..7$, and indexed by the triplet of $(i,f,t)$, field, filter, and time. To make a decision at $t$, the scheduler computes all of the seven features for all of the $(i,f)$ pairs. There are some features that don't change in every time-step, for instance, if $i$ is not visited at $t^j$, then $f_5(i,f,t^j)= f_5(i,f,t^{j+1})$. For such cases, the implementation has a categorized updating routines to avoid redundant computations. {\bf For features such as the cloud coverage that are inherently random variables, we have separate predictive modules that evaluate the expectation of their values at the time of the next observation. \bf}


\begin{table}
\caption{Features of the approximated Markovian model for the telescope-environment system. Features $f_{1\dots 7}$ provide a memoryless, approximate description of the system's state.}
\begin{tabular}{|l|l|}
\hline
Notation & Definition\textbackslash Description\\ \hline \hline 
$f_1(i,f,t)$ & \pbox{0.85\textwidth}{($slew(id(t),i)+settling(id(t),i)+\Delta t_{f} I_{ft(t) \neq f} \vee t_{dome}(i)$): either the time required to point the telescope to $i$, and change the filter to $f$, or the time required to relocate the dome to make $i$ visible. Whichever that is larger.}\\ \hline
$f_2(i,f,t)$ & \pbox{0.86\textwidth}{the total number of the same-night visits of field-filter $(i,f)$ until $t$.}\\ \hline
$f_3(i,f,t)$ &  \pbox{0.86\textwidth}{$(t - \tau_n(i,f,t)) I_{\{\theta(i,f,t) > \tau_s(t)\}}$, time since the last same-night visit of $(f,i).$}\\ \hline
$f_4(i,f,t)$ &  \pbox{0.86\textwidth}{remaining time for field-filter $(i,f)$ to become invisible, either by passing the airmass or the moon-separation limit, or being covered by temporary objects such as clouds, as projected at $t$.}\\ \hline
$f_5(i,f,t)$ &  \pbox{0.86\textwidth}{co-added depth, a measure of cumulative quality of past visits of field-filter$(i,f)$ until $t$.}\\ \hline
$f_6(i,f,t)$ &  \pbox{0.86\textwidth}{$5\sigma$-depth, a measure for quality of visiting field-filter $(i,f)$ at $t$, depending on seeing, sky brightness, and airmass. $f_6(i,f,t) = C_m + 2.5 \log (\frac{0.7}{\sigma(i,t)}) + 0.50 (br(i,t)-21) - K(i,f) am(i,t)$ where $C_m$ is a scaling coefficient.}\\ \hline
$f_7(i,t)$  &  \pbox{0.85\textwidth}{hour angle of field $i$ at $t$.}\\ \hline
\hline
\end{tabular}
\end{table}\label{tab_features}

{\bf The main computational time during the real-time scheduling is spent on the evaluation of the features (evaluating the policy, is only a number of multiplications and summations). Some features such as the slew times are essentially look up tables. Some features, such as the co-added depth are stored in the memory, indexed by a pair of field-filter, which the scheduler updates for only one field-filter after every decision step. And the rest are continuous variables such as the location of the clouds which can be interpolated from their past values, even if the exact value at a given time is not available. \bf}

\subsection{Basis functions of the telescope scheduler}\label{sec_lsst_bfs}
Basis functions, $\Phi_k$ for $k = 1 \dots 5$, are fully determined by the value of the features. Hence, they are indexed in the same way that features are, by a triplet of (i,f,t), field, filter, and time. Similar to the update of the features, all five basis functions should be evaluated for all pairs of $(i,f)$ for a decision at time $t$. Except that in this case, admissibility of a field-filter can be evaluated because it only depends on the values of its features which are already computed at this stage (see Section~\ref{sec_cstr} for the list of constraints which ensure admisibility). While it is required to evaluate the features for all possible pairs of $(i,f)$ at all decision steps, the number of the basis function evaluations is on average a factor of three times less than the number of the feature computations.

\textit{Common basis functions} are shared amongst all of the regions of the sky. They are designed to reflect the five general decision factors described in Section \ref{sec_lsst_problem}. The exact definitions of the common basis functions are presented in Table~\ref{tab_commonBF}, and the key terms and notations can be found in Table \ref{tab_notatopn}. 

\begin{table}[h]
\caption{Basis functions of the Feature-Based scheduler, the building blocks of the decision function.}
\begin{tabular}{| l | l |}
\hline
Notation & Definition\textbackslash Description\\ \hline \hline
$\Phi_1(f_1(i,f,t))$ & \pbox{0.84\textwidth}{$s_1.f_1(i,f,t)$, the cost of the required time for visiting field-filter $(i,f)$. {\bf  scale factor, $s_1$, is chosen to be 0.43. This is based on the values of $f_1$ in our simulations, and is chosen to ensure that 80\% of the $\Phi_1$ values are between 0 and 1. Scaling is a regulation that improves the rate of training convergence \bf}}\\ \hline
$\Phi_2(f_2(i,f,t))$ &\pbox{0.84\textwidth}{$\begin{cases}0.5,& \text{if } \sum\limits_{f}{f_2(i,f,t)} = 0\\ 1,& \text{if } \sum\limits_{f}{f_2(i,f,t)} = 2\\ 0,  & \text{else,}\end{cases}$, \newline reflects the short term visit/revisit priority of field $i$, conditioned on the total number of the previous same-night visits.}\\ \hline
$\Phi_3(f_5(i,t))$ &  \pbox{0.84\textwidth}{$(1 - \frac{f_5(i,f,t)}{\max_\iota \max_\phi f_4(\iota,\phi,t)})$, reflects the long-term visit priority of field-filter $(i,f)$, based on the ratio of its co-added depth to the maximum co-added depth of all pairs of field-filter until $t$.}\\ \hline
$\Phi_4(f_6(i,f,t))$ &\pbox{0.84\textwidth}{ $1 - Pr(f_6(\iota,\phi,t) \leq f_6(i,f,t))$, empirical complimentary CDF of $5\sigma$-depth of all $(i,f)$ pairs at $t$. $\Phi_4$ assigns a cost to field-filter $(i,f)$ based on its relative visiting quality compared to the other field-filter pairs at $t$.}\\ \hline
$\Phi_5(f_6(i,f,t))$ &  \pbox{0.84\textwidth}{$\frac{|hr(i,t)|}{12}$, encourages visiting of the fields near the meridian.}\\ \hline
\end{tabular}
\end{table}\label{tab_commonBF}

The LSST's mission poses different requirements on different regions of the sky, shown in Figure \ref{fig_proposals}. First we modify $\Phi_2$, for the Wide Fast Deep (WFD) and North Ecliptic Spur (NES) regions. Because they require the telescope to observe a field twice at a same night\footnote{Later, during the development of the scheduler software it was decided that this constraint should be applied to all of the regions.} within a valid time window, $[W_1,W_2]$. The following modification prioritizes the fields that have received a first visit, but not a second visit at the same night. There are two regimes to prioritize these fields, encoded in $\mathds{1}_{(f_4 \geq W_2)}$. This function is zero if the expected remaining time for a field to become invisible, $f_4$, is less than $W_2$, hence the associated cost is minimal (=0) to ensure that this field will receive its second visit before it becomes invisible. If $f_4$ is larger than or equal $W_2$ it means that the expected remaining time for the field to become invisible is longer than the revisit deadline, therefore $\mathds{1}_{(f_4 \geq W_2)}$ is one and the cost is evaluated by a different regime defined by $\Phi^{\text{pair}}$ in the following.
%
\begin{equation*}
\Phi_2^{\text{WFD}}(f_2,f_3,f_4) =\begin{cases} \Phi^{\text{pair}}(f_3) \mathds{1}_{(f_4 \geq W_2)},& \text{if } \sum\limits_{f}{f_2} = 1,\\ \Phi_2(f_2),& \text{else,} \end{cases}
\end{equation*}
%
where, $\Phi^{\text{pair}}(f_3) = \exp(- \frac{\min_{\phi}f_3(i,\phi,t)}{W_2})$. Fields that have received their first visit of the night are distinguished via $\sum\limits_{f}{f_2} = 1$, sum of the number of visits in all filters, then are prioritize amongst themselves by $\Phi^{\text{pair}}$. This function assigns a smaller cost to those fields that their first visit occurred earlier than the others.

Deep Drilling Field (DDF) region contains a very small fraction of the visible sky's area (it is about 10 individual fields). Hence it is unnecessary to adjust the basis functions that yield a separate generic policy for such fields. Instead, we treat the observation of DDFs as interruptions to the scheduler's regular operation, with each interruption comprising a sequence of DDF observations. This scheme is computationally more efficient and reduces the structural complexity of the scheduler. The recoverability attribute of the Feature-Based scheduler enables the scheme of the interruptions to be a part of the optimal scheduling.

\subsubsection{Controllability of the scheduler}\label{sec_sim_cont}
As discussed in Section~\ref{sec_SM}, the scheduler is optimizable if the telescope is controllable given a structure for the scheduler. In this section we discuss the empirical controllability of the LSST given the Feature-based scheduler. What we observe is the variations of two sample objective functions with respect to the variations of the design parameters, $\theta$. If there is no meaningful variations then the telescope is not controllable with this scheduler. As a result, the scheduler is not amenable to any form of optimization. On the other hand, if the objective function is extremely variable with respect to the changes in the design parameters, the solution of the optimization is not fully reliable, because the objective is not a well-behaved function of the optimization variables.

In order to observe the variability of the objective functions with respect to changes in the scheduler's design parameters, first we defined a sequence of equidistant values for $\theta_i \in \{ 0, 0.5, \dots, 8.5, 9\}$ and kept the other $\theta$'s fixed at 2 or 5 or 8 in separate experiments. Then we scheduled a simulated episode of observation for $t_n - t_0 = 4.8$ hours, for each $\theta_i$ separately. Finally, we evaluated the objective functions $U_1$ and $U_2$ of each schedule. Where,
%
\begin{equation}\label{equ_short_term_U1}
U_1(x_0,x_{1}, \dots, x_{n})= \sum_{\{i:t_0<t^i<t_n\}} {- slew(id(t_{i}), id(t_i)) -  10 am(id(t_i))},
\end{equation}
%
\begin{equation}\label{equ_short_term_U2}
U_2(x_0,x_{1}, \dots, x_{n})= n.
\end{equation}
%
The first objective function reflects the slew time, $slew()$, and airmass, $am()$, aversion, and $U_2$ reflects the time-efficiency of the operation by counting the total number of observations. Note that the second objective function belongs to the class of black-box functions, and the first one belongs to the other class where there exist a decomposition based on the discounted sum of the rewards.

\begin{figure}[h]
\begin{center}$
\begin{array}{ll}
\includegraphics[width=.5\linewidth]{Theta1.pdf}&
\includegraphics[width=.5\linewidth]{Theta2.pdf}
\end{array}$
\end{center}

\begin{center}$
\begin{array}{ll}
\includegraphics[width=.5\linewidth]{Theta3.pdf}&
\includegraphics[width=.5\linewidth]{Theta4.pdf}
\end{array}$
\end{center}

\begin{center}$
\begin{array}{l}
\includegraphics[width=.5\linewidth]{Theta5.pdf}
\end{array}$
\end{center}

\caption{One dimensional slices of two objective functions, $U_1$ and $U_2$, defined in Equations (\ref{equ_short_term_U1}) and (\ref{equ_short_term_U2}) respectively. The variation of the objective functions, specially in the mid-range slices (solid line) suggests that the LSST is controllable with the Feature-Based Scheduler.}
\label{fig_controlability}
\end{figure}

Figure~\ref{fig_controlability}, contains slices of the 5-dimensional $U_1$ and $U_2$. Both of the simple objective functions reasonably respond to the changes in all five dimensions of the variable $\theta$, which can be an evidence of the controllability. Moreover, the smaller variations for slices closer to the boundaries of the search space, suggest that the design and scaling of the basis functions provide a desirable behavior within the proposed search space.

\subsection{Survey-specific constraints}\label{sec_cstr}

The scheduler's decision at each time-step is an admissible (feasible and measurable) pair of field-filter $(i,f)$. Feasibility of a candidate $(i,f)$ is driven by the following factors:

\begin{itemize}
\item Visibility: The candidate field-filter has to be visible.
\item Quality: The expected observational quality of a field-filter, such as the expected depth of the resulting image, has to be better than a given threshold.
\item Survey's timing: The science-driven revisit constraints has to be respected.
\end{itemize}

To ensure measurability, the above criteria must be evaluated based only on the information that is encoded in the feature-space. Exact expressions of the proposed constraints for the LSST scheduler are presented in Table~\ref{tab_feasibility}.

\begin{table}
\caption{Feasibility of field-filter $(i,f)$ for a visit at $t^{n+1}$. We use the probabilistic expectation, E[.], of the stochastic values, as evaluated at $t^n$.}
\begin{tabular}{| l | l | l | l |}
\hline
& Constraints& Description & region\\ \hline \hline

1& \pbox{0.3\textwidth}{$\tau_{rise}(i,f,t^{n+1}) \leq t^{n+1} \leq \tau_{set}(i,f,t^{n+1}) $ }& \pbox{0.5\textwidth}{field-filter $(i,f)$ has to be above the acceptable airmass horizon at $t^{n+1}.$ }& All regions\\ \hline

2& \pbox{0.3\textwidth}{$ E[f_4(i,f,t^{n+1})] \neq 0 $ }& \pbox{0.5\textwidth}{field-filter $(i,f)$ is not temporarily masked (e.g. by the moon) at $t^{n+1}$. }& All regions\\ \hline

3 &  \pbox{0.3\textwidth}{$\sum_{f}f_2(i,f,t^n) < N^{\textit{survey}}$ }& \pbox{0.5\textwidth}{$N^{\textit{survey}}$ poses a region dependent upper-bound on the number of the visits for each field. $N^{WFD}= N^{NES} = 3$, and $N^{GPR}= N^{SCP} = 1.$ }& All regions\\ \hline

4 &  \pbox{0.3\textwidth}{$E[f_6(i,f,t^{n+1})] < \sigma({\textit{survey},f})$ }& \pbox{0.5\textwidth}{the expected quality of visiting field-filter $(i,f)$ at $t^{n+1}$ has to be better than the given threshold, $\sigma(.)$, that depends on the survey and the filter. }& All regions\\ \hline

5&  \pbox{0.3\textwidth}{$f \neq id(t^n)$ }& \pbox{0.5\textwidth}{consecutive visit of a same field is not allowed. }& All except DDF\\ \hline

6&  \pbox{0.3\textwidth}{if $\sum_{f}f_2(i,f,t^n) = 0$ then \newline $\max_\phi f_4(i,\phi,t^n) > \frac{W_1+W_2}{2}$ }& \pbox{0.5\textwidth}{the first visit of field $f$ has to occur $\frac{W_1+W_2}{2}$ time before it becomes invisible, so that the second visit of $f$ can be scheduled in the valid time window. }& WFD and NES\footnote{Later, during the development of the scheduler software it was decided that this constraint as well as the 7th constraint should be applied to all of the regions.} \\ \hline

7&  \pbox{0.3\textwidth}{if $\max_{\phi}\theta(i,\phi,t^n) > \tau_s(t^n)$ then \newline $ W_1 \leq \min_{\phi}f_2(i,\phi,t^n) \leq W_2 $}& \pbox{0.5\textwidth}{if there has been a same-night visit of field $f$ until $t^n$, then the next same-night visit has to occur in the valid time window. }& WFD and NES\\ \hline

8&  \pbox{0.3\textwidth}{if \newline $\max_{\phi}\theta(i,\phi,t^n) > \tau_s(t^n)$ then \newline $f \notin \{\text{y},\text{u}\}$}& \pbox{0.5\textwidth}{if there is a same-night visit of field $f$ until $t^n$, then the next same-night visit cannot be with either of u or y filters. }& WFD\\ \hline

9&  \pbox{0.3\textwidth}{$f \notin \{\text{y},\text{u}\}$ }&  \pbox{0.5\textwidth}{visits with u filter and y filter is not allowed. }& NES\\ \hline

\end{tabular}
\end{table}\label{tab_feasibility}

\subsection{Scheduler optimization}\label{sec_lsst_opt}

In this Section, we present two simple choices for the high-level mission objective to demonstrate the application of the proposed optimization approaches, discussed in Section~\ref{sec_opt}. (More sophisticated mission objective functions can be defined based on the LSST performance studies such as \citep{2016AJ.151..172G}, \citep{2018AJ.155..1G}, \citep{2017AJ.153..186J}, and \citep{2012AJ.144..9O}). The choice of optimization algorithm depends on the nature of the mission objective. The first experimental mission objective function in this section can be expressed as the discounted sum of instant rewards $R(s_{i-1}, s_i)$, thus the reinforcement learning is applied to find the scheduler's parameters $\theta$. The second objective function cannot be decomposed as a discounted sum of instant rewards, thus we used the global optimizer approach. From the computational point of view, the first approach is preferred. For the following experiment, the reinforcement learning is about 10 times faster than the global optimization, and requires 50 times less memory. From the practical point of view however, for some missions, it is impossible to define an objective function that can be expressed as discounted sum of a well-defined instant reward. In which case, the mission objective can be optimized only via global optimizer.

In the following experiments, for both of the optimizations we used a simulated model of the telescope \citep{2014SPIE.9150E..14C} and the environment, including the brightness of the sky and coverage of the clouds which are developed based on the measurements at the LSST site. 

\subsubsection{Reinforcement learning for the first choice of mission objective.} 
Let the instant reward, $R(i-1,i)$, be $-slew(id(t_{i-1}), id(t_{i})) - am(id(t_i))$. It is defined as a linear combination of the slew time to point the telescope from the $(i-1)$th field to the $i$th field, and the airmass of the destination. Since both factors have negative effect on the quality of the observation, the reward would be measured by the negative of each. Then the mission objective function can be simply defined as  $\sum_{i=0}^N \gamma^i R(i-1, i)$.

The simulation for the reinforcement learning starts at $t_0 = 2462867.5~mjd$ (2021 January 1), with $\theta^0 = (5,5,5,5,5)$, initialized at the mid-range values, and continues until $\theta$ converges. Figure~\ref{fig_theta_conv}, is the training curve for all of the variables over a course of 3000 decisions, . The discount rate $\gamma = 0.9$, and learning rate $\frac{0.01}{\log^3(i)}$ are chosen empirically. 

\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.5\linewidth]{TDcurve.pdf}
\end{center}
\caption{Reinforcement learning of the scheduler's parameters, $(\theta_1,\theta_2,\theta_3,\theta_4,\theta_5)$. All of the parameters are initialized at the mid-range value, 5. During the simulation, after each decision step there is a reward associated with the outcome of the decision which implies a small adjustment on each of the five design parameters. Then the next decision will be taken with a slightly different set of parameters. This procedure continues until the adjustments on the variables are negligible.}
\label{fig_theta_conv}
\end{figure}

For the above choices of reward, learning rate, discount factor and initialization, $\theta$ converges to $\theta^* = (8.18,  1.04,  3.26,  7.59,  1.13)$. With a personal computer\footnote{Processor: 1.6 GHz, Memory 1600 MHz DDR3}, each decision and its associated update takes about $0.8$ sec\footnote{This reflects the efficiency of the first scheduler's prototype. Time of each decision simulation for the current efficient software is about 0.036 sec.}, thus the time of the convergence for the simulation, presented in this section is $3000\times 0.8 \text{ sec } = 40 \text{ minutes }$. Note that the optimization time is linear with respect to the number of decision steps.

\subsubsection{global optimization for the second choice of mission objective.} 
One of the important simple objective functions that cannot be expressed as the discounted sum of the rewards is the total number of the observations from a given $t_i$ to a given $t_j$ which can be expressed as, $U_{\pi_{\theta}}(x_i, x_{i+1}, \dots, x_j) = j - i$. To find a set of parameters, $\theta$, that optimizes this objective function we applied the global optimization approach, explained in Section \ref{sec_opt}. To decrease the computational time, we used the following regulatory constraints,
%
\begin{itemize}
\item$\theta \geq 0$: Positive coefficients for the basis functions are assumed in the design of the basis functions. Because in the context of the telescope scheduling, it is more natural to create the basis functions to reflect the cost of the operation.
\item $\theta_1 =\theta_0$: Without loss of generality, we fix the value of the first element of $\theta$ to reduce the dimension of the optimization problem by one. Because, homogeneity of the policy implies that if $\theta^*$ yields an optimal scheduler, then $\alpha \theta^*$ for $\alpha > 0$ yields an optimal scheduler as well.
\end{itemize}

We defined the above objective function, $U_{\pi_{\theta}}$, over a period of 10 days, from $t_i = 2462867.5~mjd$ (2021 January 1) to $t_j = 2462877.5~mjd$ (2021 January 11). Figure~\ref{fig_eDEObjectiveFunction}, shows the value of this objective function at each iteration of the $e$DE algorithm. The solution $\theta^* = (1.00, 0.84, 0.99,  1.34,  3.04)$, yields the best $U_{\pi_{\theta}}$ after 50 iterations.
%
\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.5\linewidth]{eDEObjectiveFunction.pdf}
\caption{Progress of the black-box objective function $U_{\pi_{\theta}}$ over the iterations of the $e$DE algorithm. $U_{\pi_{\theta}}$, for this simulation is the total number of the observations for 10 nights starting from (2021 January 1).}
\label{fig_eDEObjectiveFunction}
\end{center}
\end{figure}

$e$DE is a population-based metaheuristic algorithm. For our experiment, shown in Figure~\ref{fig_eDEObjectiveFunction}, the number of population $N_P$ is set to be 50. Each function evaluation is in fact, the simulated operation of the telescope for 10 nights, with a candidate scheduler which takes about 8 minutes. Therefore each iteration of the algorithm $e$DE, takes $N_P * 8$ minutes\footnote{Processor: 1.6 GHz, Memory 1600 MHz DDR3}. The optimization can be manually terminated if the result is satisfactory, or can be continued until a full convergence is achieved. In $e$DE (and all genetic algorithms in general), function evaluation for each individual is independent from other individuals, therefore the parallel implementation of the same algorithm can be faster up to a factor of $N_p$. 
 
\section{Performance of a modified Feature-Based scheduler for LSST}\label{sec_comp}
In this section, the LSST Metric Analysis Framework \citep{jones2014lsst} is used, to compare\footnote{The sky background models and weather downtime used to benchmark the algorithms are not exactly identical because of the practical difficulties in the separation of the environment and the opsim scheduler. However, for the purpose of the comparisons in this section, the behavior of our sky and observatory model is sufficiently close to the official model. See \citep{2016SPIE.9910E..13D}, and \citep{2016SPIE.9911E..25R} for the official operation simulator.} the performance of a modified version of the Feature-Based scheduler with opsim V4 and opsim V3, the most recent producers of the baseline schedules of the LSST. (We use two specific sequences produced by each of these scheduler. Namely, astro-lsst-01-2013 and minion-1016 produced respectively by opsim V4 and opsim V3.)

{\bf The following results are based on the simulations that use a computer model of the LSST-environment. First we optimized the scheduler with the model over a limited simulated episode of observation. Then we used the optimal scheduler to schedule the same model of the LSST-environment within a simulated period of time from 2021 to 2031 for the purpose of performance evaluation. The resulting schedule however, will not be used in practice, because it is made by some realization of the random processes such as the cloud coverage. For the real-life operation of LSST, we use the same approach to optimize the scheduler, however the decisions are made on-the-fly, using the real-time values of the features. 

Recall that the optimization part consumes the main portion of the total computational time. Fortunately, the scheduler's optimization is an offline process, and can be done before the telescope is ready to operate. On the other hand, the real-time decision making, with an already optimal scheduler is a very fast process. It only includes evaluation of the features and basis functions, in addition to basic summation and sorting operations. The current version of the Feature-Based scheduler can schedule 28 observations per second, while the length of each observation in real-time is at least 30 seconds. Hence, the decision making speed is roughly 900 times faster than what is required for the real-time scheduling. However, we are still working to increase the time efficiency of the software, because it directly effects the computational time of the offline optimization procedure.\bf}

The \textit{Modified Feature-Based} scheduler is under active development\footnote{GitHub repository: \url{https://github.com/lsst/sims_featureScheduler}.}, and addresses the observational details of the LSST's mission through the adjustment of the constraints and the basis functions. It is designed to produce a software that can be used in practice. 

Unlike the default sky tessellation for LSST, we do not require the tessellation centers to be determined based on the telescope's field of view. The default sky tessellation adopted in the baseline scheduler results in 23\% of the sky being covered by more than one field which causes serious non-uniformity in the final coverage of the sky. In the modified Feature-Based scheduler, we adopt a 50 times finer discretization of the sky using Hierarchical Equal Area isoLatitude Pixelization (HEALpix) \citep{gorski2005healpix}. The fact that the decision-making is not computationally expensive makes it possible to use a much finer discretization of the feature-space resulted by the finer tessellation of the sky. This approach allows the scheduler to handle the overlaps which cause inhomogeneity of the final sky coverage. 

In addition to adopting a finer discretization, we use a spatial dithering scheme to randomize the final pointing of the telescope by a small amount around the tessellation centers to further assist the homogeneity of the coverage. Adopting the dithering scheme, the median number of observations at a typical point in the sky increases by $\sim15$\%. Dithering is also essential for removing systematic effects for science cases such as measuring galaxy counts, (see \citep{Awan2016} for more details). Moreover, the modified Feature-Based scheduler uses separate routines to decide if an observation will need to be observed in a pair, and to decide if a Deep Drilling sequence should be executed by interrupting the normal operation of the telescope.

\subsection{Sky coverage uniformity}
For a survey telescope, such as the LSST, the density of the co-added depth over the visible sky should ideally be uniform in each filter and within each of the five survey regions. Figure~\ref{fig_10yrs_skymap} compares the values of the co-added depth on a discretized full sky map. Figure \ref{fig_zoomin_r}, demonstrates the smoothness of the coverage by zoom-in to a smaller area of the sky map. It shows the co-added depth values of opsim V4, with and without dithering, and compare them with that of the modified Feature-Based scheduler in the \textit{r} band. The zoomed-in area is around the boundary of WFD and GP regions which have different target co-added depth. Smoother coverage that the modified Feature-Based scheduler offers is due to the fine discretization of the sky in the decision making stage, in addition to the dithering which takes place after the decision is made.

Figure \ref{fig_10yrs_hist} compare the distribution of the co-added depth on a (finely) discretized sky. The modified Feature-Based scheduler has paved the left-most peak that appears in the distribution of opsim V4. This peak is the result of the field's undesirable overlaps. Table \ref{table_10yrs_hist} contains the median and variance of the co-added depth for both of the schedules in each of the sky regions and in each filter. The modified Feature-Based scheduler provides deeper (higher median), and more uniform (lower variance) coverage in most of the cases.

\begin{figure}[h!]
\plottwo{OpSim10yrs_skymap.pdf}{FB10yrs_skymap.pdf}
\caption{Sky coverage in each of the six filters $\{ u, g, r, i, z, y\}$, measured by co-added depth. According to the mission's objective, the scheduler has to provide a uniform coverage of the visible sky within each region, and in each filter. The left panels show the opsim V4 simulation results while the modified Feature-Based scheduler is on the right. Even without a given observation proposal, modified Feature-Based scheduler can closely match the large-scale footprint of the official survey. (The individual yellow dots with high co-added depth are fields in deep drilling survey, and the NES region is not visited in the $u$, and $y$ bands.)}
\label{fig_10yrs_skymap}
\end{figure}


\begin{figure}
\epsscale{0.35}
\plotone{undithered_zoomed3.pdf}
\plotone{dithered_zoomed3.pdf}
\plotone{fb_zoomed3.pdf}
\epsscale{1}
\caption{Co-added depth coverage in a zoomed-in area of the sky map, around the border between the WFD (green) and SCP (blue) regions, in the \textit{r} band. The smooth coverage of the modified Feature-Based scheduler (right) versus the granular pattern of opsim V4 (left), further respects the uniformity of the coverage which is one of the most fundamental objectives of the LSST mission. The middle figure is the coverage of opsim V4, with dithering of the same sequence of observations which fundamentally can not become as smooth as the right figure, because unlike Feature-Based scheduler, the scheme of opsim V4 does not easily allow for decision making with arbitrarily fine tesselations of the sky.}\label{fig_zoomin_r}
\end{figure}



\begin{figure}[h!]
\centering
\includegraphics[width=1.0\linewidth]{Co_addedHist10yrs.pdf}
\caption{Each plot compares the distributions of the co-added depth coverage in one of the six filters. A dithering scheme in the modified Feature-Based scheduler in addition to a finer tessellation of the sky smoothens the density of the coverage where the fields overlap.}
\label{fig_10yrs_hist}
\end{figure}


\begin{table}
\caption{The median and variance of the co-added depth distribution on a finely discretized sky. modified Feature-Based scheduler closely matches the footprint of the official survey, and in addition outperforms opsim V4 in terms of the uniformity of the coverage, with lower variances, specially in WFD and SCP regions.}\label{table_10yrs_hist}
\begin{center}
\begin{tabular}{l|cccc|cccc} \hline
&\multicolumn{8}{c}{Median, Variance} \\ \hline
&\multicolumn{4}{c|}{opsim V4} &\multicolumn{4}{c}{Modified Feature-Based} \\
filter  & WFD & GP & SCP & NES & WFD & GP & SCP & NES\\
\hline
\textit{u}& 25.63, 0.04 & 25.12, 0.11& 24.91, 0.12& - &25.68, 0.01 &25.32, 0.05 & 25.10, 0.04& -\\
\textit{g}& 27.13, 0.04 & 26.41, 0.09& 26.32, 0.12 & 26.30,0.13& 27.18, 0.01 & 26.69, 0.04&26.56, 0.04 &26.47, 0.09 \\
\textit{r}& 27.19,0.04 & 26.01,0.16& 25.84,0.24 & 26.38, 0.12 & 27.14, 0.01& 26.21, 0.07 & 26.08, 0.05 & 26.43, 0.09\\
\textit{i}& 26.60, 0.04 & 25.44, 0.15 & 25.28, 0.22 & 25.82, 0.12 & 26.56, 0.01 & 25.68, 0.08 & 25.43, 0.07 & 25.88, 0.09\\
\textit{z}& 25.73, 0.04& 24.62,0.17& 24.57, 0.21& 24.90, 0.14 & 25.87, 0.01 & 25.03, 0.09 & 24.81, 0.05 & 25.16, 0.10\\
\textit{y}& 24.92, 0.04 & 23.81, 0.16 & 23.72, 0.21& - & 24.92, 0.02 & 24.01, 0.09 &23.88, 0.06 & -\\
%all & 157.2, 1.37&151.4, 11.94& 150.6, 6.32&103.4, 11.1&157.4, 0.38 & 153.0, 2.12&151.9, 1.56& 104.0, 1.40\\
\end{tabular}
\end{center}
\end{table}

%%%%%LSST's mission calls for uniform coverage of the sky and prefers a smaller variance in the distribution of the final co-added depth. 
\subsection{Pairs}
In addition to uniformity of the coverage, the LSST mission calls for pairs of visits within a valid time window at the same night. The main reason is to detect the transient objects such as asteroids. Because, the moving objects usually belong to the solar system, the pair constraint was initially imposed only on the WFD and NES regions. However, there are interesting solar system objects such as interstellar asteroids that can be observed in any direction of the sky. In addition, identification of the other varying objects, such as super novae, can benefit from a follow up visit, especially if the second visit is with a different filter. Thus in the modified Feature-Based scheduler we made the pair constraint a universal constraint for all of the regions. The downside of this extension is the fact that it constrains the scheduler even more and the performance can be potentially less than it could be. 

Note that the structure of the Feature-Based scheduler, allows for extension or restriction of the constraints down to the level of an individual field-filter, with neither contradicting any of the Markovian assumptions, nor breaking the structure of the implementation. 

Figure~\ref{fig_10yrs_pair} demonstrates the distribution of observations in pairs (in the $g$, $r$, and $i$ filters) to the total number of the observations. For the regions that the pair constraint is applied, this ratio can be interpreted as the success rate of the schedulers in satisfying the pair constraint. This success rate ranges from 0 to 1, and the higher values indicate the more successful pair visits.

 Figure~\ref{fig_10yrs_pair_hist}, compares the distribution of the pairs ratio of the modified Feature-Based scheduler and opsim V4 on a (finely) discretized sky. Note that the peak of the distribution for the modified Feature-Based scheduler is closer to $1$, which means a larger area of the sky is covered by successful pairs visit, however opsim V4 offers a sharper concentration of the values that can be interpreted as a more homogenous pairs visit which agrees more with the LSST's mission to survey the sky in a uniform manner.
 %
\begin{figure}[h!]
\plottwo{OpSim10yrs_Pair_skymap.pdf}{FB10yrs_Pair_skymap.pdf}
\caption{The ratio of the pairs (in the $g$, $r$, and $i$ filters) to the total number of the observations on the sky map. For the areas that the pair constraint is applied, the ratio is desired to be one. For modified Feature-Based scheduler (right), the pair constraint is applied to all of the regions and for opsim V4 (left), it is applied to the WFD and NES regions only.}
\label{fig_10yrs_pair}
\end{figure}
%
\begin{figure}[h!]
\centering
\includegraphics[width=.4\linewidth]{PairHist.pdf}
\caption{Distribution of the pairs ratio to the total number of the observations, in the $g$, $r$, and $i$ filters. The Feature-Based scheduler covers a large area of the sky by successful pairs visit (higher median). And opsim V4, maintains a uniform ratio of pairs for a larger area of the sky (lower variance).}
\label{fig_10yrs_pair_hist}
\end{figure}

\subsection{AltAz and airmass distributions}
Airmass is one of the major obstacles for ground-based instruments. Zenith observations have the minimum airmass, however off-zenith observations cannot be avoided, due to time-efficiency and design limitations. In which case, observations around the meridian provide high-quality images and consequently result in more efficient operation of the instrument. Figure~\ref{fig_10yrs_AltAz} compares the number density of the visits on an altitude-azimuth sky map in each of the six filters $[u,g,r,i,z,y]$. Clearly in all of the filters, the modified Feature-Based scheduler schedules more visits around the meridian zone. In addition, it offers a consistent concentration peak on the east wings, which is essential for a higher success rate of the pairs constraint. Because, if the first visit of the night occurs when the field is on the east side of the sky, the scheduler has a longer opportunity to schedule the second visit of the night, hence it is more likely to perform a successful pair visit. Figure \ref{fig_all_alt_az} demonstrates the density of visits collectively in all filters for opsim V3, opsim V4, and the modified Feature-Based scheduler. Note that the adjustability of the Feature-Based scheduler allows for a significant change in the behavior of the telescope, in this case, to encourage observations around the meridian we defined a new basis function in such a way that the modified Feature-Based scheduler prefers to observe a contiguous set of fields that is then re-observed later in the same order.
%
\begin{figure}[h!]
\begin{center}$
\begin{array}{rl cc rl}
\includegraphics[width=.25\linewidth]{opsim_Nvisits_as_function_of_Alt_Az_u_HEAL_SkyMap.pdf}&
\includegraphics[width=.25\linewidth]{opsim_Nvisits_as_function_of_Alt_Az_g_HEAL_SkyMap.pdf}&    &  &
\includegraphics[width=.25\linewidth]{FB_Nvisits_as_function_of_Alt_Az_u_HEAL_SkyMap.pdf}&
\includegraphics[width=.25\linewidth]{FB_Nvisits_as_function_of_Alt_Az_g_HEAL_SkyMap.pdf}\\
\includegraphics[width=.25\linewidth]{opsim_Nvisits_as_function_of_Alt_Az_r_HEAL_SkyMap.pdf}&
\includegraphics[width=.25\linewidth]{opsim_Nvisits_as_function_of_Alt_Az_i_HEAL_SkyMap.pdf}&    &  &
\includegraphics[width=.25\linewidth]{FB_Nvisits_as_function_of_Alt_Az_r_HEAL_SkyMap.pdf}&
\includegraphics[width=.25\linewidth]{FB_Nvisits_as_function_of_Alt_Az_i_HEAL_SkyMap.pdf}\\
\includegraphics[width=.25\linewidth]{opsim_Nvisits_as_function_of_Alt_Az_z_HEAL_SkyMap.pdf}&
\includegraphics[width=.25\linewidth]{opsim_Nvisits_as_function_of_Alt_Az_y_HEAL_SkyMap.pdf}&    &  &
\includegraphics[width=.25\linewidth]{FB_Nvisits_as_function_of_Alt_Az_z_HEAL_SkyMap.pdf}&
\includegraphics[width=.25\linewidth]{FB_Nvisits_as_function_of_Alt_Az_y_HEAL_SkyMap.pdf}\\
\end{array}$
\end{center} 
\caption{Each plot is the distribution of the visits on an altitude-azimuth sky map in one of the six filters. The two left columns belong to opsim V4, and the two right columns belong to the modified Feature-Based scheduler. The higher concentration on the meridian (vertical axis) for the modified Feature-Based scheduler shows a more desirable behavior. Moreover, consistent concentration of the visits on the east wing can potentially provide a better success rate in pairs observation.}
\label{fig_10yrs_AltAz}
\end{figure}

\subsection{Signal-to-Noise ratio}
For a multi-objective survey telescope, such as LSST, comparing the overall performance of the different schedules is a difficult task. Particularly because of the large number of competing factors that are involved in the evaluation of their performance. In some cases these criteria are not even objective or well-defined, such as the importance of an area of astronomy compared to the rest. Nevertheless we conclude this section with a general comparison of the performances using the overall signal-to-noise ratio of the schedules. Table~\ref{OSF_table} reflects the value of median throughput for three different schedulers in \textit{r} and \textit{g} bands. The modified Feature-Based scheduler significantly outperforms the other two. 

In addition note that the throughput is mainly determined by the combination of open shutter fraction (OSF), and airmass. The open shutter fraction is the total time that the telescope camera shutter was open divided by the maximal time it could have been open. This reveals how time-efficiently the observations have been scheduled. The median airmass reflects the overall quality of the collected data. As mentioned before, observations in lower airmass allows for a higher data quality. Comparing the values of the OSF and airmass for both of the baseline schedules, opsim V3 and opsim V4, shows that there is a trade-off between the two values. While opsim V4 offers better median airmass, its OSF is worse. However, its median throughput is very close to that of opsim V3. This comparison reveals that the change of meta-parameters, and tweaking the structures of proposal-based schedulers such as opsim V3 and opsim V4, only changes the balance of trade-off between OSF and airmass, but not the actual performance of the scheduling.

\begin{table}
\caption{Comparison of the schedulers in a section of the LSST main survey area, WFD}\label{OSF_table}
\begin{center}
\begin{tabular}{l|ccccc} \hline
Survey &\multicolumn{2}{c}{median throughput} & OSF & median Airmass & dithered\\
&$r$ (\%) &  $g$ (\%) &&&\\ \hline
modified Feature-Based & 63.7 & 47.0& 0.705 & 1.1 & yes  \\
opsim V3      & 55.3 & 40.0 & 0.736 & 1.2 & no  \\
opsim V4      & 54.4 & 40.8 & 0.715 & 1.1 & no \\ \hline
\end{tabular}
\end{center}
\end{table}



% Figures generated by https://github.com/yoachim/18_scratch/blob/master/bosf_values/BOSF_check.ipynb
\begin{figure}
\epsscale{0.35}
\plotone{minion_1016_Observation_Density_HEAL_SkyMap.pdf}
\plotone{astro-lsst-01_2013_Observation_Density_HEAL_SkyMap.pdf}
\plotone{FB_Observation_Density_HEAL_SkyMap.pdf}
\epsscale{1}
\caption{The distribution of the visits on an altitude-azimuth sky map for three schedulers. On the left, opsim V3 schedules most observations at high airmass and very few on the meridian. In the middle, opsim V4 schedules many observations on the meridian, but still executes deep drilling fields and a smattering of other observations at high airmass. On the right, the modified Feature-Based scheduler concentrates observations around the meridian, including deep drilling observation. }\label{fig_all_alt_az}
\end{figure}

\section{Concluding remarks}\label{sec_conclusion}

This study demonstrated that a Markovian scheduler with expert-designed features, and a parametrized linear decision-making policy can be successfully applied to multi-mission, ground-based telescopes such as LSST. Unlike the mainstream telescope schedulers, Feature-Based scheduler does not rely on hand-crafted observation proposals. Instead, by bringing the decision making process to the individual observation level improves the efficiency of the telescope's operation. 

In particular, while proposals are designed for feasibility, our approach is designed for optimality in addition to feasibility. In general, automatic schedulers such as the Feature-Based scheduler, are fundamentally less prone to suboptimality compare to the schedulers that heavily rely on human interaction. This is mainly due to the manual tailoring which is performed based on the inspections of the instances. Moreover, adjusting the behavior of human-dependent schedulers are inconvenient and time consuming in practice. Furthermore, being modeled as a Markovian Decision Process, the Feature-Based scheduler is a systematic approach to operate the instrument under uncertainties and interruptions.

On the other hand, the building blocks of the Feature-Based scheduler are designed modularly in an intuitive way for the astronomy community. This property allows for expert intervention if needed. However, the robust structure and implementation of the scheduler, allows regulated interventions only. In particular, valid interventions include defining new features, new basis functions, and adjusting the design parameters, and none of which breaks measurability, linearity, and memorylessness of the scheduler. As a result, all of the desirable properties such as simplicity, optimality conditions ,and modularity of the design remain valid. 

In addition, due to the coherent structure, from training to online decision-making, the Feature-Based scheduler is easy to understand, implement, and troubleshoot. Simplicity of the design and implementation, also provide a user-friendly environment for a wide-range of programming expertise in astronomy community to define a custom mission objective, train a scheduler and examine the behavior of the scheduler with various mission objectives. Similarly, in a particular project, when a change in the mission's objective is necessary, deriving a new scheduler that optimizes the new objective is principally automated. Furthermore, for the mission planning stage of a future instrument, a scheduler with adjustable objective can be extremely helpful, because it can answer the high-level trade-off questions, such as the time efficiency of the different proposed strategies.

Computationally, the optimization part of the scheduler is the most demanding task. Once the optimal values of the design parameters are known, one night of observation can be simulated in 2-3 minutes. Equivalently, in real-time operations, decision are made on-the-fly about 900 faster than what is needed. For the optimization part however, the necessary computational time and resources vary. If many different objective functions are being tested for planning a mission, then a quick $e$DE optimization, that takes a few hours, can find a sufficiently good scheduler for each mission. Even a quick manual hand tuning that reflects the intuitive importance of each basis function is possible, because they carry an astronomical meaning. On the other hand if the objective is known and fixed, and the scheduler is being trained for real-time decision-making, then one might even categorize the observation nights based on their main differences, such as the moon-phase, seasonal variations, and weather patterns, then trains a scheduler specifically for each category to further increase the efficiency of the operation. 


\appendix

\begin{proof} (of Proposition \ref{prop_main})
Consider a function $C_{\pi}: \pazocal{S} \rightarrow \rm I\!R$, defined as follows,
%
\begin{equation*}
\begin{aligned}
C_{\pi}(x_{i}) &= -E_{\pi}[ \sum_{i\leq j}^N \gamma^{i - j} R_{\pi(X_j)}(X_{j}, X_{{j+1}}) |x_{i}]\\
& = - E_{\pi}[R_{\pi(x_i)}(x_{i}, X_{{i+1}}) | x_{i}] - \gamma E_{\pi}[ \sum_{ i+1 \leq j}^N \gamma^{j-(i+1)} R_{\pi(X_j)}(X_{j}, X_{j+1}) | x_{i}].\\
\end{aligned}
\end{equation*}
%
Where $E_{\pi}[.]$ is the expectation of the argument inside the brackets, if policy $\pi$ is being used. Then by applying the law of total expectation on the second term,
\begin{equation*}
\begin{aligned}
C_{\pi}(x_{{i}}) &= - E_{\pi}[R_{\pi(x_i)}(x_{i}, X_{{i+1}}) | x_{i}] - \gamma E_{\pi}[ E_{\pi}[ \sum_{ i+1 \leq j}^N R_{\pi(X_j)}(X_{j}, X_{{j+1}}) |X_{{i+1}}]| x_{i}]\\
&=  - E_{\pi}[R_{\pi(x_i)}(x_{i}, X_{{i+1}}) | x_{i}] - \gamma E_{\pi}[ C_{\pi}({X_{{i+1}}})| x_{i}].
\end{aligned}
\end{equation*}
By assuming a finite state space one can expand the expectation as a finite sum,
\begin{equation*}
C_{\pi}(x_{{i}}) =  - E_{\pi}[R_{\pi(x_i)}(x_{i}, X_{{i+1}}) | x_{i}]  - \gamma \sum_{x_{i+1} \in \pazocal{S}} \mathbb P(x_{i+1}|\pi(x_i), x_{i}, x_{i-1},\dots, x_{0}) C_{\pi}({x_{{i+1}}}).\\
\end{equation*}
Then by the Markov property, probabilities that are conditioned on the whole previous sequence of states can be replaced by probabilities that are conditioned only on the previous state,
\begin{equation*}
\begin{aligned}
C_{\pi}(x_{i}) &=  - E_{\pi}[R_{\pi(x_i)}(x_{i}, X_{{i+1}}) | x_{i}] - \gamma \sum_{x_{i+1} \in \pazocal{S}} P_{\pi(x_{i})}(x_{i},x_{i+1}) C_{\pi}({x_{{i+1}}}),\\
\end{aligned}
\end{equation*}
where, $P_{\pi(x_{i})}(x_{i},x_{i+1}) $ is the transition probability from $x_i$ to $x_{i+1}$, under the outcome action of policy $\pi$. 
Now, let $C^*(x_{i}) = \min_{\pi} C_{\pi}(x_{{i}})$ then,
\begin{equation}\label{equ_proof1}
\begin{aligned}
C^{*}(x_{{i}}) &=  ~~~~~~~ \min_{\pi} ( - E_{\pi}[R_{\pi(x_i)}(x_{i}, X_{{i+1}}) | x_{i}] - \gamma \sum_{x_{i+1} \in \pazocal{S}} P_{\pi(x_{i})}(x_{i},x_{i+1}) C_{\pi}({x_{{i+1}}}) ) \\
& =  \min_{\{\pi(x_{i}), \pi(x_{{i+1}}),\dots \}}(- E_{\pi}[R_{\pi(x_i)}(x_{i}, X_{{i+1}}) | x_{i}] - \gamma \sum_{x_{i+1} \in \pazocal{S}} P_{\pi(x_{i})}(x_{i},x_{i+1}) C_{\pi}({x_{{i+1}}}))\\
& = ~~~~~~~ \min_{\pi(x_{i})} (- E_{\pi}[R_{\pi(x_i)}(x_{i}, X_{{i+1}}) | x_{i}] - \min_{\{\pi(x_{i+1}), \pi(x_{{i+2}}),\dots \}} \gamma \sum_{x_{i+1} \in \pazocal{S}} P_{\pi(x_{i})}(x_{i+1},x_{i}) C_{\pi}({x_{{i+1}}})).\\
\end{aligned}
\end{equation}
%
For the next time-step, one can construct a function, $\hat {C}$,  such that, $\hat C(x_{{i+1}}) = \min_{\pi(x_{{i+1}})} \min_{\pi} C_{\pi} ({x_{i+1}})$, then, 
%
\begin{equation*}
\begin{aligned}
\sum_{x_{i+1} \in \pazocal{S}} P_{\pi(x_{i})}(x_{i},x_{i+1}) \min_{\{\pi(x_{i+1}), \pi(x_{{i+2}}),\dots \}} C_{\pi}({x_{{i+1}}})&= \sum_{x_{i+1} \in \pazocal{S}} P_{\pi(x_{i})}(x_{i},x_{i+1})  \hat C ({x_{{i+1}}} )\\
& \geq \min_{\pi} \sum_{x_{i+1} \in \pazocal{S}} P_{\pi(x_{i})}(x_{i+1},x_{i}) C_{\pi}({x_{{i+1}}}).
\end{aligned}
\end{equation*}
On the other hand, 
\begin{equation*}
\sum_{x_{i+1} \in \pazocal{S}} P_{\pi(x_{i})}(x_{i},x_{i+1}) \min_{\{\pi(x_{i+1}), \pi(x_{{i+2}}),\dots \}} C_{\pi}({x_{{i+1}}}) \leq \min_{\pi} \sum_{x_{i+1} \in \pazocal{S}} P_{\pi(x_{i})}(x_{i},x_{i+1}) C_{\pi}({x_{{i+1}}}).
\end{equation*}
Therefore,
\begin{equation*}
 \min_{\pi} \sum_{x_{i+1} \in \pazocal{S}} P_{\pi(x_{i})}(x_{i},x_{i+1}) C_{\pi}({x_{{i+1}}})=\sum_{x_{i+1} \in \pazocal{S}} P_{\pi(x_{i})}(x_{i},x_{i+1}) \min_{\{\pi(x_{i+1}), \pi(x_{{i+2}}),\dots \}} C_{\pi}({x_{{i+1}}}).
\end{equation*}
By substituting the second term of the right hand side of Equation (\ref{equ_proof1}) with the right hand side of the above equation,
\begin{equation*}
\begin{aligned}
C^{*}(x_{{i}}) &= \min_{\pi(x_{i})} (- E_{\pi}[R_{\pi(x_i)}(x_{i}, X_{{i+1}}) | x_{i}] -\gamma \sum_{x_{i+1} \in \pazocal{S}} P_{\pi(x_i)}(x_{i},x_{i+1}) \min_{\pi} C_{\pi}({x_{{i+1}}}))\\
&= \min_{\pi(x_{i})} (- E_{\pi}[R_{\pi(x_i)}(x_{i}, X_{{i+1}}) | x_{i}] -\gamma \sum_{x_{i+1} \in \pazocal{S}} P_{\pi(x_i)}(x_{i},x_{i+1}) C^*({x_{{i+1}}})).\\
\end{aligned}
\end{equation*}
The last equality follows from the definition of $C^*$, and is in the form of Optimal Bellman Equation, for which a solution exists [Bellman 1957]. Moreover $C^*(x_0) = \min{C^\pi(x_0)}$ attains the optimal value by the construction of $C^\pi(x_0)$ which is equal to $- E[\sum _{\pi(x_i)}(x_{{i}}, X_{i+1}) | x_0]$.

Now, given a $C^*$, an optimal policy can be simply evaluated,
\begin{equation}\label{equ_proof2}
\begin{aligned}
\pi^*(x_{i}) & = \argmin_{a_{i} \in \pazocal{A}_i} (- E_{\pi}[R_{a_i}(x_{{i}}, X_{i+1}) | x_{i}] -\gamma \sum_{x_{i+1} \in \pazocal{S}} P_{a_i}(x_{i},x_{i+1})C^*({x_{{i+1}}}))\\
& = \argmin_{a_{i} \in \pazocal{A}_i} (- E_{\pi}[R_{a_i}(x_{{i}}, X_{i+1}) - \gamma C^*({X_{{i+1}}})| x_{i}]).\\
\end{aligned}
\end{equation}
Finally define,
\begin{equation}\label{equ_phi}
\Phi(X_{{i+1}}) := R_{a_i}(x_{{i}}, X_{i+1}) - \gamma C^*({X_{{i+1}}}),
\end{equation}
 and substitute $ R_{a_i}(x_{{i}}, X_{i+1}) - \gamma C^*({X_{{i+1}}}) $ with $\Phi(X_{{i+1}}) $ in Equation (\ref{equ_proof2}) to complete the proof.
\end{proof}
%
\acknowledgments
\textbf{Acknowledgments.} We would like to thank the LSST team, and the DIRAC Institute's faculty and researchers for providing expertise without which this research would not have been possible. In particular, Professor \v{Z}eljko Ivezi\'{c} whose consistent attention and insightful comments greatly assisted this work. We would also like to show our gratitude to the experts in other institutes, specially Professor Michael Strauss, Professor Christopher Stubbs, Dr. Robert Lupton, and Dr. Michael Reuter for their insightful comments spanning from the idea stage to the end of this project. We are also grateful for the time that the anonymous referees took to provide both high-level and detailed comments which significantly improved the clarity of the paper. This work is financially supported by the National Science Foundation under Cooperative Agreement 1258333 managed by the Association of Universities for Research in Astronomy (AURA), and the Department of Energy under Contract No. DE-AC02-76SF00515 with the SLAC National Accelerator Laboratory. Additional LSST funding comes from private donations, grants to universities, and in-kind support from LSSTC Institutional Members. The DIRAC Institute is supported through generous gifts from the Charles and Lisa Simonyi Fund for Arts and Sciences, and the Washington Research Foundation.

\facility{LSST}

\software{MAF \citep{jones2014lsst}, healpy \citep{Healpy05}, matplotlib \citep{matplotlib07}, 
          astropy \citep{astropy18}, numpy/scipy \citep{scipy}}

%\bibliographystyle{unsrt}
%\bibliography{/Users/elahesadatnaghib/Dropbox/Graduate/Research/Princeton/Publications/references}
\bibliography{references}
\end{document}


